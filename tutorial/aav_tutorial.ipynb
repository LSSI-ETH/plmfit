{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAV Dataset Tutorial: Embedding and Feature Extraction with PLMFit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to use `plmfit` as a library to process protein sequences from the AAV (Adeno-Associated Virus) dataset. We'll cover:\n",
    "1. Loading and preparing the AAV dataset\n",
    "2. Extracting protein sequence embeddings using pre-trained language models\n",
    "3. Training a downstream model on these embeddings for property prediction\n",
    "\n",
    "The AAV dataset contains engineered variants of the adeno-associated virus (AAV) capsid protein, which is widely used in gene therapy. By analyzing these variants, we can predict their properties and understand sequence-function relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "We'll start by importing the necessary modules and setting up our environment. The key components we'll use are:\n",
    "\n",
    "- `extract_embeddings`: For generating protein sequence embeddings\n",
    "- `feature_extraction`: For training downstream prediction models\n",
    "- `Logger`: For tracking experiments and results\n",
    "- `utils`: For data loading and helper functions\n",
    "- `DefaultArgs`: For managing configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the project root to the Python path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from plmfit.functions.extract_embeddings import extract_embeddings\n",
    "from plmfit.functions.feature_extraction import feature_extraction\n",
    "from plmfit.logger import Logger\n",
    "from plmfit.shared_utils import utils\n",
    "from plmfit.args_parser import DefaultArgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the AAV Dataset\n",
    "The AAV dataset contains engineered variants of the AAV capsid protein, each with:\n",
    "\n",
    "A unique sequence identifier\n",
    "The protein sequence\n",
    "Associated functional measurements\n",
    "Let's load and examine the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../tutorial/aav_data_sample.csv'\n",
    "aav_sample_df = utils.load_dataset(data_path)\n",
    "print(f'Dataset shape: {aav_sample_df.shape}')\n",
    "aav_sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Embedding Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Extract Embeddings?\n",
    "Protein language models (like ESM) learn rich, contextual representations of protein sequences during pre-training. These embeddings capture:\n",
    "- Evolutionary information\n",
    "- Structural properties\n",
    "- Functional characteristics\n",
    "\n",
    "We'll use the ESM-2 model with 8M parameters, which provides is the smallest model available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize default arguments\n",
    "args = DefaultArgs()\n",
    "\n",
    "# Configure embedding extraction parameters\n",
    "args.plm = \"esm2_t6_8M_UR50D\"  # Small but effective model\n",
    "args.layer = \"quarter1\"  # Early layer for general features\n",
    "args.reduction = \"mean\"  # Average over sequence positions\n",
    "args.batch_size = 32  # Smaller batch size for memory efficiency\n",
    "args.output_dir = \"../tutorial\"\n",
    "args.experiment_dir = f\"{args.output_dir}/aav_tutorial\"\n",
    "args.experiment_name = f\"aav_sample_{args.plm}_{args.layer}_{args.reduction}\"\n",
    "\n",
    "# Initialize logger for tracking\n",
    "logger = Logger(experiment_name=args.experiment_name, base_dir=args.experiment_dir)\n",
    "\n",
    "embeddings_path = f\"{args.experiment_dir}/{args.experiment_name}.pt\"\n",
    "\n",
    "print(\"Starting embedding extraction...\")\n",
    "extract_embeddings(args, logger, data=aav_sample_df)\n",
    "print(\"Embedding extraction complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's happening:\n",
    "\n",
    "The model processes each protein sequence through the ESM-2 network\n",
    "For each sequence, it extracts the hidden representations from the specified layer\n",
    "These representations are averaged across the sequence length\n",
    "The resulting embeddings are saved to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have rich protein representations, we'll train a simple neural network to predict protein properties from these embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model architecture and training parameters\n",
    "args = DefaultArgs()\n",
    "args.head_config = {\n",
    "    \"architecture_parameters\": {\n",
    "        \"network_type\": \"mlp\",  # Multi-layer perceptron\n",
    "        \"output_dim\": 1,  # Single output for regression\n",
    "        \"hidden_dim\": 128,  # Hidden layer size\n",
    "        \"task\": \"regression\",  # Predicting continuous values\n",
    "        \"hidden_activation\": \"relu\",  # Non-linearity\n",
    "        \"hidden_dropout\": 0.25,  # Avoid overfitting\n",
    "    },\n",
    "    \"training_parameters\": {\n",
    "        \"learning_rate\": 0.00005,  # Small learning rate\n",
    "        \"epochs\": 200,  # Training iterations\n",
    "        \"batch_size\": 64,  # Number of samples per batch\n",
    "        \"loss_f\": \"mse\",  # Mean squared error loss\n",
    "        \"optimizer\": \"adam\",  # Adaptive learning rate optimizer\n",
    "        \"val_split\": 0.2,  # 20% of data for validation\n",
    "        \"weight_decay\": 0.01,  # L2 regularization\n",
    "        \"early_stopping\": 30,  # Stop if no improvement for 30 epochs\n",
    "    },\n",
    "}\n",
    "\n",
    "# Set up paths and run feature extraction\n",
    "args.split = \"sampled\"  # Use the sampled split (PLMFit will generate it in the background)\n",
    "args.ray_tuning = \"False\"  # Disable hyperparameter tuning for this example\n",
    "args.embeddings_path = embeddings_path  # Path to our saved embeddings\n",
    "\n",
    "print(\"Starting feature extraction and model training...\")\n",
    "feature_extraction(args, logger, data=aav_sample_df, head_config=args.head_config)\n",
    "print(\"Feature extraction and training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Model Architecture\n",
    "The downstream model consists of:\n",
    "- Input Layer: Takes the 320-dimensional protein embeddings\n",
    "- Hidden Layer: 128 neurons with ReLU activation\n",
    "- Dropout: 25% dropout for regularization\n",
    "- Output Layer: Single neuron for regression prediction\n",
    "\n",
    "### Training Process\n",
    "The model is trained using the Adam optimizer with a small learning rate\n",
    "Early stopping prevents overfitting by monitoring the validation loss\n",
    "The best model weights are saved based on validation performance\n",
    "\n",
    "### Next Steps\n",
    "After running this pipeline, you might want to:\n",
    "\n",
    "Visualize the learned embeddings using t-SNE or UMAP\n",
    "Interpret which sequence features contribute most to the predictions\n",
    "Try different model architectures or hyperparameters\n",
    "Fine-tune the model using PLMFit and parameter-efficient fine-tuning methods like LoRA\n",
    "\n",
    "### Troubleshooting\n",
    "If you encounter any issues:\n",
    "\n",
    "Ensure all file paths are correct\n",
    "Check that you have sufficient GPU memory\n",
    "Verify that all required packages are installed\n",
    "Reduce the batch size if you run into memory errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
